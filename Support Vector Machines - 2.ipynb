{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26692044-776f-4b1c-adbe-5ad5e46af5b0",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "### Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing sets\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- Use the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use\n",
    "\n",
    "**Note:** You can use any dataset of your choice for this assignment, but make sure it is suitable for classification and has a sufficient number of features and samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fb080-d15f-48e0-91c3-1c401effa27a",
   "metadata": {},
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "In machine learning algorithms, particularly in Support Vector Machines (SVMs), polynomial functions are used as kernel functions to enable non-linear classification. The kernel trick allows SVMs to operate in a higher-dimensional feature space without the need to compute the actual transformation explicitly. This is achieved by using a polynomial kernel function, which computes the inner product in the higher-dimensional space directly.\n",
    "\n",
    "The polynomial kernel function is given by:\n",
    "\n",
    "\\[\n",
    "K(x, y) = (x \\cdot y + c)^d\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(x\\) and \\(y\\) are input feature vectors,\n",
    "- \\(c\\) is a constant (usually set to 0),\n",
    "- \\(d\\) is the degree of the polynomial.\n",
    "\n",
    "This kernel function transforms the data into a higher-dimensional space, making it easier to find a linear separating hyperplane for non-linearly separable data.\n",
    "\n",
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "To implement an SVM with a polynomial kernel in Python using Scikit-learn, follow these steps:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with polynomial kernel\n",
    "svm_clf = SVC(kernel='poly', degree=3, coef0=1)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy of the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece51c4-09d2-42e0-ba7f-79839f331e14",
   "metadata": {},
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "In Support Vector Regression (SVR), the epsilon parameter defines a margin of tolerance, within which no penalty is given for the error. When the value of epsilon is increased, the region around the prediction function within which errors are not penalized becomes larger. This effectively reduces the number of data points that fall outside the epsilon margin, leading to fewer support vectors being selected. As a result, increasing epsilon typically reduces the complexity of the model by allowing more data points to be within the margin, leading to fewer support vectors.\n",
    "\n",
    "However, increasing epsilon too much may result in underfitting, as the model may become too simple and unable to capture the nuances in the data.\n",
    "\n",
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)?\n",
    "\n",
    "### 1. **Kernel Function**:\n",
    "The kernel function defines the type of transformation applied to the input data to make it suitable for separation in a higher-dimensional space. Common kernel functions include:\n",
    "- **Linear Kernel**: Used when the data is already linearly separable. It works well when the data shows a linear relationship.\n",
    "- **Polynomial Kernel**: Useful when the data is non-linear but can be transformed to a higher-dimensional space where it becomes linearly separable. The degree of the polynomial controls the complexity of the decision boundary.\n",
    "- **Radial Basis Function (RBF) Kernel**: This is the most commonly used kernel in SVR. It maps data to an infinite-dimensional space, making it suitable for highly complex, non-linear relationships. The performance of this kernel heavily depends on the choice of the gamma parameter.\n",
    "\n",
    "### 2. **C Parameter**:\n",
    "The C parameter is a regularization parameter that controls the trade-off between achieving a low error on the training data and maintaining a simple model. \n",
    "- A **high value of C** leads to a smaller margin but fewer misclassification errors, possibly leading to overfitting.\n",
    "- A **low value of C** results in a larger margin but more errors, which could lead to underfitting.\n",
    "\n",
    "### 3. **Epsilon Parameter**:\n",
    "The epsilon parameter defines the width of the margin of tolerance. Increasing epsilon:\n",
    "- Reduces the number of support vectors, leading to a simpler model.\n",
    "- Increases the tolerance for errors, which could result in underfitting if set too high.\n",
    "- A smaller epsilon allows the model to fit the data more closely, potentially capturing more complex relationships but at the risk of overfitting.\n",
    "\n",
    "### 4. **Gamma Parameter**:\n",
    "The gamma parameter defines the influence of a single training example. It controls how far the influence of a single training example reaches.\n",
    "- A **high value of gamma** means that each data point has a very localized influence, leading to a more complex model and a risk of overfitting.\n",
    "- A **low value of gamma** means that the influence of each data point extends over a larger region, leading to a smoother decision boundary and potentially underfitting.\n",
    "\n",
    "### Example:\n",
    "- If your data is linearly separable, you might choose a **linear kernel**, a **high C** (to avoid misclassification), and a **large epsilon** (to avoid overfitting).\n",
    "- For non-linear data, the **RBF kernel** with a **lower C** and an appropriately tuned **gamma** may work better.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6597b3-d90a-4645-ba6a-eedd4fed6309",
   "metadata": {},
   "source": [
    "## Q5. SVM Classification Assignment\n",
    "\n",
    "## Objective:\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing sets\n",
    "- Preprocess the data using any technique of your choice (e.g., scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- Use the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g., accuracy, precision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use\n",
    "\n",
    "## Solution:\n",
    "\n",
    "```python\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load the dataset (using the Iris dataset for this example)\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Labels\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (scaling the features using StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it\n",
    "svm_clf = SVC(kernel='linear', C=1)\n",
    "svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Predict the labels of the testing data\n",
    "y_pred = svm_clf.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier (using accuracy and classification report)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Tune hyperparameters using GridSearchCV (e.g., tuning 'C' and 'gamma')\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "best_svm_clf = grid_search.best_estimator_\n",
    "best_svm_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 9: Predict again and evaluate the tuned model\n",
    "y_pred_tuned = best_svm_clf.predict(X_test_scaled)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "print(f\"Accuracy after hyperparameter tuning: {accuracy_tuned:.2f}\")\n",
    "print(\"Tuned Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Step 10: Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svm_clf, 'svm_classifier_model.pkl')\n",
    "joblib.dump(scaler, 'scaler_model.pkl')\n",
    "\n",
    "print(\"Model and scaler saved successfully.\")\n",
    "```\n",
    "\n",
    "## Explanation:\n",
    "1. **Importing Libraries:** We import all necessary libraries such as SVC from sklearn.svm, train_test_split, GridSearchCV, StandardScaler, etc.\n",
    "2. **Loading Dataset:** We use the load_iris() function to load the Iris dataset. You can replace it with any other dataset suitable for classification.\n",
    "3. **Splitting Dataset:** The dataset is split into training and testing sets using train_test_split.\n",
    "4. **Preprocessing:** We scale the features using StandardScaler to normalize the data, ensuring the SVM model performs well.\n",
    "5. **Training the Model:** An instance of SVC with a linear kernel is created and trained on the scaled training data.\n",
    "6. **Prediction and Evaluation**: We predict the labels for the testing set and evaluate the model's performance using accuracy and classification report metrics.\n",
    "7. **Hyperparameter Tuning:** We use GridSearchCV to tune the hyperparameters (C, gamma, and kernel) to improve the model's performance.\n",
    "8. **Re-training the Tuned Model:** The best model from the grid search is retrained on the entire dataset.\n",
    "9. **Final Evaluation:** The accuracy and classification report of the tuned model are printed.\n",
    "10. **Saving the Model:** We save both the trained model and the scaler using joblib.dump so they can be used for predictions in the future.\n",
    "\n",
    "## Conclusion:\n",
    "- This solution demonstrates how to train a classification model using SVM, tune its hyperparameters, and save the trained model for future use.\n",
    "- You can replace the Iris dataset with any other classification dataset by modifying the data loading part.\n",
    "- The model can later be reloaded and used for prediction by using joblib.load('svm_classifier_model.pkl') for the classifier and joblib.load('scaler_model.pkl') for scaling new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
